---
title: "Analyzing Data from the Midfield Project"
author: "Javier Alvarez, Leah Hunt, and Joshua Kwak"
output: pdf_document
---

```{r imports, echo=FALSE, include=FALSE}
library(tidyr)
library(data.table)
library(midfielddata)
library(Metrics)
library(caret)
library(rcompanion)
library(ggplot2)
library(hasseDiagram)
library(kableExtra)
library(cowplot)
```

#Introduction
##Background
The Multiple-Institution Database for Investigating Engineering Longitudinal Development
(MIDFIELD) is a database that contains population data for 2.2 million undergraduates at 20
higher education institutions with engineering programs. MIDFIELD contains a substantial
amount of information about students including their race, gender, high school GPA, degree
major, and much more. By using the MIDFIELD database to identify patterns and trends as well
as find evidence for statistical research questions, we gain a better understanding of student
performances in school and encourage other researchers to conduct studies using the database.
MIDFIELD continues to work towards improving data validation methods as well as expanding
the number of institutions included in the database. In our case, we will be working with a
selection of the MIDFIELD data set compiled by Purdue University researchers on 97,640
students from 12 institutions during the years 1987 to 2016.

##Statistical Research Question
In this experiment, we are interested in looking at how the proportion of women in STEM has
changed over the year by school. To do this, we will split the time period of 1987 – 2016 into X
equal increments and observe the differences in the proportion of women pursuing STEM
degrees by school. This makes ANOVA a perfect tool to use for this research question because
we can compare the sample arithmetic means of each of those time periods and identify if there
is a relation between the proportion of women in STEM and school attended. Put formally, the
statistical research question is: How has the proportion of women pursuing STEM degrees
changed over time.


Additionally, we want to observe how race and gender affects how well the course Calculus I acts
as a “weed-out” class. Namely, how does race and gender affect the rate of students deciding to
quit pursuing a STEM field and pursue something different after taking Calculus I. To do this,
we will look at the number of students who have dropped out of a STEM degree within one
semester of taking Calc I as compared to the total number of students who have taken Calc I. By
using a logistic regression model, we will model the probability of a student dropping out of a
STEM degree, and we will use race and gender as inputs.

##Discussion of Models
The Hasse Diagram is given in Figure 1.
```{r hasse, echo=FALSE, fig.cap="Hasse Diagram", fig.align='center', fig.height=4, fig.width=4}
ALabs <- c("1 Grand Mean 1", " Year ",
              " (Institution) ", " (Error) ")
AMat <- matrix(data = F, nrow = 4, ncol = 4)
AMat[1, c(2:4)] = AMat[c(2:3), 4] = T
hasseDiagram::hasse(AMat, ALabs)
```

The null and alternative hypothesis are formally expressed below.

Where $Y_{ij}$ represents the proportion of women in STEM during time period i receiving treatment
j

##Methods
###Study Design
In our study, the response will be the proportion of women graduating with STEM degrees, and
the measurement units will be the 12 institutions given in the MIDFIELD database. Because the
treatments are applied to the institutions, the 12 institutions are also the experimental units.
The factor is each incremental time period of X years between 1987 and 2016 which means there
are Y treatment levels. Because we are using an in-subject repeated measures design, the block will be the same as the subject, the institution.

The institutions that we got the data about the proportion of women in STEM from were
randomly chosen for each year. Each institution randomly selected a subset of its population,
and we measured the proportion of women in that population. It is relevant to note that the
number of institutions measured differs per year so there is not an even distribution of data
points. Although the institutions were randomized to account for this fact, it does provide a
potential source of error/inaccuracy of the model in our ANOVA analysis.

###Type I Error Rate
We have decided that we will control Experimentwise Error Rate at the 0.10 level for both
components of our study (ANOVA and Logistic Regression portions). Given the low-stakes
nature of our study, we believed it was reasonable to use a more liberal Type I Error control
method. Having a type I error would not cause a significant impact nor incur overwhelming
costs, so having a low evidentiary requirement is justified. We plan to control EER with the
Protected LSD method.

##Exploratoring the Data
###Attributes
There are two attributes we are concerned about in this study: the time periods in which we are
interested and the proportion of women pursuing STEM. Because the question we are interested
in is the change in the proportion of women over time, graphing the data will provide some
general useful insights.

Figure 2 and 3 gives plots of the data for all institutions over the entire time period. While there
are clearly more data available in the earlier years, it does appear that there is an upward trend
in the proportion of women in STEM. The boxplot graph gives a clearer look at how the sample
arithmetic means have steadily been increasing as the years increased, but it does show that
there is variation and that the growth is strictly linear. Because of this, we would expect that
there is a difference in the sample arithmetic means in the proportion of women in STEM
between different time periods. We also see that the data is not evenly spread out which
indicates that the data is not homoscedastic and may require transformation. ANOVA analysis
will provide greater insight into these details.

```{r dataprep, echo=FALSE, include=FALSE}
m_f<-read.csv("master_anova.csv")
tpx <- m_f[,1]
instx <- m_f[,2]
px <- m_f[,3]
df <- data.frame(tpx,instx, px)
df

  tp <- df[,1]
inst <- df[,2]
p <- df[,3]
df2 <- data.frame(tp,p)
df2$tp <- as.factor(df2$tp)
df2$p <- as.factor(df2$p)

#Prepping Time Periods

TimePeriod <- c(rep("1989 - 1993", 31), rep("1994 - 1998", 50), 
                rep("1999 - 2003", 50), rep("2004 - 2008", 14), 
                rep("2009 - 2016", 18))
PropWomen <- df[,3]
df3 <- data.frame(TimePeriod, PropWomen)
df3
```


```{r strip, echo=FALSE, out.width=".49\\linewidth", fig.cap="Stripchart and Boxplot of Proportion of Women over Time", fig.show = 'hold'}
stripchart(p~tp, vertical = TRUE, 
           pch = 20, ylab = "Proportion of Women", 
           xlab = "School Year", 
           main = "Stripchart of Women in STEM")

#FILL IN REAL BOXPLOT CODE HERE!!!
plot(p~tp, 
     pch = 20, 
     ylab = "Proportion of Women", 
     xlab = "School Year", 
     main = "Boxplot of Women in STEM")

```


Table 1 gives the values of various descriptive statistics for the proportion of women pursuing STEM degrees by time periods.

#Exploring the Change in the Proportion of Women Graduates in STEM Majors over Time



#Exploring the Effect of Race and Gender on Drop Out Rates after Introductory Calculus
The second portion to our analysis considers the role of the introductory calculus course as a weed out course for STEM majors. Introductory calculus is a requirement for nearly all STEM majors and is one of the most commonly cited reasons for students to leave the STEM major. \cite{paper} While works such as that done by Rasmussen and Ellis have considered factors impacting the probability of continuing the calculus curriculum after the first calculus course, we wish to consider factors that impact students' probability of leaving the STEM major after introductory calculus, in particular to see how the probability varies by race and gender. 

To consider this problem, we use the Midfield data to identify STEM students who took Calculus I and label them as either having dropped out of the STEM field in association with Calculus I or not having dropped out of the STEM field in association with introductory calculus.\footnote{A more detailed description of the data wrangling techniques and the methods by which the drop out variable was created can be found in Appendix A.} We then develop a logistic regression model to model each students' probability of dropping out after introductory calculus based on their race and gender in order to test our hypothesis that at least one of race and gender affect a student's probability of dropping out of the STEM program after introductory calculus against the null hypothesis that neither is a significant factor. 

##Exploratory Analysis
We begin by visualizaing the collected dropout data.
```{r dinput, echo=FALSE, include=FALSE}
#Load data for logistic regression analysis
train<-fread("master_glm.csv")
train<-train[race!="Unknown" & sex!="Unknown"]
train<-train[,race:=ifelse(race=="International", "Other", race)]

```

```{r datavis, echo=FALSE, include=FALSE}
# Create prep data for EDA plot
trainp<-train[,race:=ifelse(race=="Native American", "Nat. Am.", race)]
trainp<-trainp[race!="White"]
trainp<-trainp[,sex:=ifelse(sex=="Male", "M", "F")]
trainp<-trainp[,dropout:=ifelse(dropout==1, "Yes", "No")]

# Component of plot for minority groups
gg1<-ggplot(data=trainp, aes(x=sex, fill = as.factor(dropout))) +  
  geom_bar() + 
  facet_grid (~race) + 
  theme(legend.position="top") + 
  labs(fill="Dropout") 

# Component of plot for white group
trainp<-train[race=="White"]
trainp<-trainp[,sex:=ifelse(sex=="Male", "M", "F")]
gg2<-ggplot(data=trainp, aes(x=sex, fill = as.factor(dropout))) +
  geom_bar() + 
  facet_grid (~race) + 
  theme(legend.position = "none") 

# Combine plots
plot_grid(gg1, gg2, rel_widths = c(3.5,1))

# Correct training data to be ready for the rest of the analysis
train[,race:=ifelse(race=="Nat. Am.","Native American", race)]
train$sex<-as.factor(train$sex)
train$race<-as.factor(train$race)

```

Viewing the above visualization, we see that we are treating gender as a factor, for which we will assume only two levels. Race is a factor which we will treat as having six levels: white, asian, black, hispanic, native american, and other minorities. Note that the other minority group also contains the category international, which was included in the midfield dataset but regrouped to other minorities for this analysis. It is also worth considering the wide variety in sample size between the different categories, with white having an overwhelming majority and Native American having only barely enough to be considered as an individual category.   

Considering this visualization relative to our subject, we see that females and several minority groups appear to be overall underrepresented in this data. Females also appear to generally have higher numbers of dropouts relative to population size for most races.

##Assumption Testing
Logistic regression requires five assumptions to be satisfied: appropriate outcome structure, independence of observations, absence of multicollinearity, linearity of independent vars and log odds, and sufficiently large sample size. \cite{glmassumptions} The first and last of these are clearly met as we designed the dependent variable to be binary and the size of the sample considered is over 8,000 students.\footnote{The full Midfield dataset contains over 97,000 students. The smaller number used in the analysis is a result of filtering the students to STEM students who were identified as having taken an introductory calculus course.} 

NEED TO DO FOURTH ASSUMPTION...maybe


```{r start_glm, echo=FALSE, include=FALSE}
# Set order for race and gender so that white men will be the baseline
Gender <- factor(train$sex, levels=c('Male', 'Female'))
Race<-factor(train$race, 
             levels = c("White", "Asian", "Black", "Hispanic",
                        "International", "Native American", "Other"))
glm_model<-glm(dropout ~ Gender + Race, family = binomial, data = train)

# Check Multicollinearity
car::vif(glm_model)

```
To consider multicollinearity, we can use the variance inflation factor (vif). The calculated values for vif were 1.009 for each predictor, which is less than 4, the commonly used indicator of collinearity problems. Therefore, we may proceed under the assumption that this assumption holds.


The last assumption, and most tricky to consider, is the independence of observations. While each student's measurements are taken individually without regard to any other student in the study, our data does not give any way for us to know that students, particularly in the same institution at the same time period, interact with or influence each other in a meaningful way. However, it is reasonable to assume that if these influences exist, they would be negligible, particularly considering that we are considering students' choices in educational field which would rarely be impacted by infuences from the choices of the other student.

Therefore, we decide that all of our assumptions have been sufficiently met, so we may proceed with our analysis.

##Developing the Model
In developing the logistic regression model, we will use a baseline of a white male. The resulting model is shown in the below table.


```{r odds_ratios, echo=FALSE, warning=FALSE, out.width=".49\\linewidth", fig.show="hold"}
# Calculate Odds Ratios
oddsrat <- exp(parameters::model_parameters(glm_model)[2])
p <- parameters::model_parameters(glm_model)["p"]
dat <- data.frame(Odds_Ratio=oddsrat, p=p)[-1,]
rownames(dat)<-c("Female", "Asian", "Black", "Hispanic", "Native American", "Other Minorities")
setnames(dat, c("Coefficient", "p"), c("Odds Ratio", "P-value"))

# Table for Odds Ratios
knitr::kable(
  dat,
  digits = 3,
  caption = "Odds Ratios Relative to white men",
  align = c('l',rep('c',4))
) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    font_size = 12, latex_options = "HOLD_position")

```


Based on our test and the p values it produced, the only significant terms are gender, the native american race, and other minority races. The p value for the impact of race rounds to 0 in the table, implying that assuming the null hypothesis we would expect to see results at least as extreme as we did nearly none of the time. Similarly, for the Native American group, we would expect to see results this extreme only .2% of the time and for the other minority groups we would expect to see a value this extreme only 4.9% of the time. Their odds ratios, which are set using the white man as a baseline, then tell us how much more likely these students are to drop out. For example, a female is predicted to be 1.217 times more likely to drop out than a man and Native Americans are predicted to be over twice as likely to drop out after introductory calculus than white people. This tells us that our data suggests that gender does influence the impact of introductory calculus as a weed out course with significance. Race, however, only statistically significantly affects the dropout rate after calculus 1 for students of Native American heritage and those from races that did not fall into any of the given categories. We should point out, however, that the Native American group was the smallest group, containing only approximately 60 total students, so the result should be taken with caution. Similarly the group of Other Minorities, most likely contains a rather diverse group of students and therefore is difficult to interpret entirely on its own.

We can also view the predicted probability of dropping out by race and gender in the below table.


```{r pred, echo=FALSE, out.width=".49\\linewidth", fig.show="hold"}
# Get predictions for each combination of race and gender
test<-data.table(Race = c("White", "Asian", "Black", "Hispanic", "Native American", "Other",
                          "White", "Asian", "Black", "Hispanic",  "Native American", "Other"),
                 Gender=c("Male","Male","Male","Male","Male","Male",
                          "Female","Female","Female","Female","Female","Female"))
test$Prediction<-predict(glm_model,newdata = test,type="response")
preds<-data.frame(Sex = c("Male", "Female"), 
                  White = c(test$Prediction[1], test$Prediction[7]), 
                  Asian = c(test$Prediction[2], test$Prediction[8]),
                  Black = c(test$Prediction[3], test$Prediction[9]),
                  Hispanic = c(test$Prediction[4], test$Prediction[10]),
                  Nat = c(test$Prediction[5], test$Prediction[11]),
                  Other = c(test$Prediction[6], test$Prediction[12]))
setnames(preds, c("Nat", "Other", "Sex"), c("Native Americans", "Other Minorities", " "))

#Make table of predictions
knitr::kable(
  preds,
  digits = 3,
  caption = "Prediction of Drop Out Probability by Race and Gender",
  align = c('l',rep('c',6))
) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    font_size = 12, latex_options = "HOLD_position")
```
These predictions show a similar story where the females are all more likely to drop out than the men and Native Americans and Other Minorities have the largest predicted dropout rates. 

We should also consider the effect size of the model. The below table gives the values for McFadden's, Coxsnell's, and Nagelkerke's pseudo-R$^2$ statistics. 

```{r eff sizes, echo=FALSE, include=FALSE}
# Calculate effect sizes
rpt<- rcompanion::nagelkerke(glm_model)
rpt
```

```{r efntable, echo=FALSE}
effsize<-data.frame(McFadden=c(.003), CoxSnell = c(.003), Nagelkerke = c(.005))

# Table for Effect Sizes
knitr::kable(effsize, 
             caption = "Effect Sizes for the Effects of Race and Gender on Dropout Rate") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"),
                            font_size = 12, latex_options = "HOLD_position")

```

McFadden's and Coxsnell's statistics suggest that the model explains .3% of the variation while Nagelkerke's suggests that the model explains .5% of the variation. While this may seem like a very small amount, keep in mind that we are attempting to predict dropout rates based solely on race and gender. We would not expect, and certainly not hope, that these factors would explain a large amount of variation in dropout rates nor was that the intended purpose of our analysis. It is also worth noting that using a likelihood ratio test and chi$^2$ statistic, we get a p value of less than .0001, which implies that our model should still be considered statistically significant relative to the null model.


#Discussion and Further Study
From the Midfield data, we have explored two aspects of minority participation in the STEM field. The first area we explored was whether there has been a growth of the proportion of women graduating with STEM degrees. WRITE STUFF ABOUT JUST FIRST ANALYSIS HERE

The second area we explored was how the probability of dropping out of the STEM field after taking an introductory calculus course varied among different races and genders. In this case, we found that females were more likely to drop out after introductory calculus than males, and some minority groups also had a significantly higher probability of dropping out. 

These analyses combine to suggest that these aspects of women and minorities in STEM should be further studied and indicates continuing underlying biases against them. That being said, both parts of this analysis did suffer from some issues that arose from incomplete data, and it should be kept in mind that even the most recent data in the Midfield study only includes up to 2017. We also acknowledge that the original intent of the Midfield study was to target engineering, which could have impacted our results. We would suggest that more study on these matters would be useful, especially to consider more recent data that targets all of STEM instead of primarily targetting engineering. We would also suggest further study into the patterns we found, in particular to explore what factors may be impeding the movement to get women more involved in the STEM field and the causation behind the more adverse effect that calculus has on women in comparision to men as well as on only select minority groups.  

\newpage

\begin{thebibliography}{9}
\bibitem{data}
Richard Layton, Russell Long and Matthew Ohland
  (2019). midfielddata: Student Record Data for 98,000
  Undergraduates. R package version 0.1.0.
  https://github.com/MIDFIELDR/midfielddata

\bibitem{cip6}
National Center for Education Statistics. (n.d.).  Browse CIP Codes. Retrieved from https://nces.ed.gov/ipeds/cipcode/browse.aspx?y=55

\bibitem{paper}
Rasmussen, C., \& Ellis, J. (2013). Students who switch out of calculus and the reasons why they leave. In Martinez, M. \& Castro Superfine, A (Eds.). \textit{Proceedings of the 35th annual meeting of the North American Chapter of the International Group for the
Psychology of Mathematics Education} (pp. 457-464). Chicago, IL: University of Illinois at Chicago.

\bibitem{glmassumptions}
Schreiber-Gregory, D. (2018). PDF.

\bibitem{intro}
https://advances.asee.org/wp-content/uploads/vol05/issue02/Papers/AEE-18-Ohland.pdf

\end{thebibliography}

\newpage

#Appendix A: Data Wrangling Methods
  Due to the extensiveness of the data wrangling necessary for this project, we leave the description of the process to this appendix and exclude the code generating the model ready data for either component of the project from the Code Appendix. For scripts to generate the model ready data, contact the authors of this work.
  The data released by the Midfield project was originally separated into four data tables, whose names and descriptions as well as the pertinent data to this analysis pulled from them can be found in the table below. 

```{r midtables, echo=FALSE, fig.width=6}
# Create data for table on midfield data explanation
midtab<-data.frame(Table = c("midfieldterms",
                             "midfieldcourses", 
                             "midfieldstudents", 
                             "midfielddegrees"), 
                   Purpose = c("Shows the students' status by term",
                               "Shows each instance of a student taking a class",
                               "Shows personal information about each student",
                               "Shows the end degree, or lack thereof, earned by each student"),
                   UsefulData = c("Program of study by term",
                                  "Semester in which Calculus 1 was taken",
                                  "Race and Gender of each student", 
                                  "Students that earned STEM degrees"))

# Create table for midfield data explanation
knitr::kable(
  midtab,
  digits = 3,
  align = c('l',rep('c',2)), caption = "Midfield Data Tables"
) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    font_size = 12, latex_options = "HOLD_position")%>%
     column_spec(1, width = '1.1in') %>%
      column_spec(2, width = "2.5in")%>%
  column_spec(3, width = "2.5in")
```
  
  In order to properly combine this data for our analysis, there were several major obsticals to overcome. The first consideration was identifying the STEM programs in the data. This was done by translating the six digit cip codes into a binary variable that identified the code as either within STEM or not within STEM. \cite{cip6} This method was used for both portions of the analysis.

##Analysis Specific to Women in STEM
  While finding the proportion of women STEM graduates by institution by year was rather straightforward, there were a few modelling decisions regarding the data. The first decision was to exclude all data that came from a year and institution that graduated fewer than 10 STEM students in the given year. This was done with the intention of helping to improve the reliability of the data. 
  
  INSERT FINAL DECISION ON WHICH DATA TO SELECT HERE; UNLESS THIS IS GOING INTO THE ANOVA SECTION ABOVE

##Analysis Specific to Dropout Rates after Calculus 1
  The most complicated challenge that we faced was to identify which semester the student took their first calculus course. This issue primarily involves the midfieldcourses data table. There are two identifiers for course in this table: course title and course code. While the course title can rather clearly identify courses that could be considered Calculus 1, only three institutions, institutions c, d, and l, provided course titles in the data. 
  
  This leaves the course code as an identifier of course. These codes, however, vary by institution and are not standardized to match to particular courses. In order to approach this issue and allow more of the data to be used, we began cross-referencing subsets of course codes from particular institutions in order to attempt to match the anonymized data to a particular institution or a particular coding key in order to figure out which course code matches to Calculus 1. Note that in the attempt to create this matching, we excluded math courses from the matching set then ensured the predicted Calculus 1 course appeared in the data set. The institutions that were identified using this method were institutions a, b, e, h, j, and l. We were unable to identify the remaining institutions, so they were excluded from the second part of our analysis.  
  
  Another major considertation to make was how to define when a student drops out of the STEM field in association with Calculus 1. Noting that the data considers each year to contain six semesters, we defined a student to have dropped out of the STEM field in association with Calculus 1 if they do not graduate with a STEM degree and they do not appear in a STEM program in the fourth, fifth, or sixth semesters following the most recent taking of the course. Simply put, we sample the time period between six months and a year after the course and see whether or not the student is still in STEM. The consideration of whether or not the graduate in STEM prevents the accidental elimination of any student that gradutates immediately or shortly after Calculus 1 while the slight delay between the measurement era and the actual taking of the course gives time for the student to choose to drop out or change majors and for this change to be recorded. 

\newpage
#Appendix B: Code Appendix

```{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}

```