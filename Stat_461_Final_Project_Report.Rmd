---
title: "Analyzing Data from the Midfield Project"
author: "Javier Alvarez, Leah Hunt, and Joshua Kwak"
output: pdf_document
---

```{r imports, echo=FALSE, include=FALSE}
library(tidyr)
library(data.table)
library(midfielddata)
library(Metrics)
library(caret)
library(rcompanion)
library(ggplot2)
```

#Introduction



##Exploratoring the Data



#Exploring the Change in the Proportion of Women Graduates in STEM Majors over Time



#Exploring the Effect of Race and Gender on Drop Out Rates after Introductory Calculus
The second portion to our analysis considers the role of the introductory calculus course as a weed out course for STEM majors. To consider this problem, we use the Midfield data to identify STEM students who took Calculus I and label them as either having dropped out of the STEM field in association with Calculus I or not having dropped out of the STEM field in association with introductory calculus.\footnote{A more detailed description of the data wrangling techniques and the methods by which the drop out variable was created can be found in Appendix A.} We then develop a logistic regression model to model each students' probability of dropping out after introductory calculus based on their race and gender. 

##Assumption Testing
Logistic regression requires five assumptions to be satisfied: appropriate outcome structure, independence of observations, absence of multicollinearity, Linearity of independent vars and log odds, and sufficiently large sample size. The first and last of these are clearly met as we designed the dependent variable to be binary and the size of the sample considered is over 8,000 students.\footnote{The full Midfield dataset contains over 97,000 students. The smaller number used in the analysis is a result of filtering the students to STEM students who were identified as having taken an introductory calculus course.} The fourth is also trivially met as we are using all categorical variables. The remaining two assumptions, however, do need some consideration.

```{r start_glm, echo=FALSE, include=FALSE}
train<-fread("master_glm.csv")
train<-train[race!="Unknown" & sex!="Unknown"]
train$sex<-as.factor(train$sex)
train$race<-as.factor(train$race)
Gender <- factor(train$sex, levels=c('Male', 'Female'))
Race<-factor(train$race, levels=c("White", "Asian", "Black", "Hispanic", "International", "Native American", "Other"))
glm_model<-glm(dropout~Gender+Race,family=binomial, data=train)

#No Multicollinearity
car::vif(glm_model)
```
To consider multicollinearity, we can use the variance inflation factor (vif). The calculated values for vif were 1.009 for each predictor, which is less than 4, the commonly used indicator of collinearity problems. Therefore, we may proceed under the assumption that this assumption holds.

The last assumption, and most tricky to consider, is the independence of observations. While each student's measurements are taken individually without regard to any other student in the study, our data does not give any way for us to know that students, particularly in the same institution at the same time period, interact with or influence each other in a meaningful way. However, it is reasonable to assume that if these influences exist, they would be negligible, particularly considering that we are considering students' choices in educational field which would rarely be impacted by infuences from the choices of the other student.

Therefore, we decide that all of our assumptions have been sufficiently met, so we may proceed with our analysis.

##Developing the Model
In developing the logistic regression model, we will use a baseline of a white male. The resulting model is shown in the below table.
```{r glm_table, echo=FALSE, warning=FALSE}
knitr::kable(
  parameters::model_parameters(glm_model)[,c(1,2,3,6,8)],
  digits = 3,
  caption = "Results of the logistic regression model",
  align = c('l',rep('c',4))
) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    font_size = 12, latex_options = "HOLD_position")
```

We see that the only significant predictors are gender, the native american race, and other races, all three of which positively influence the dropout rate. This tells us that our data suggests that gender does influence the impact of introductory calculus as a weed out course. Race, however, only affects the dropout rate after calculus 1 for students of Native American heritage and those from races that did not fall into any of the given categories.

#References
(Format later)
Richard Layton, Russell Long and Matthew Ohland
  (2019). midfielddata: Student Record Data for 98,000
  Undergraduates. R package version 0.1.0.
  https://github.com/MIDFIELDR/midfielddata
  
https://nces.ed.gov/ipeds/cipcode/browse.aspx?y=55 (cip6 translations)  

https://www.lexjansen.com/wuss/2018/130_Final_Paper_PDF.pdf (glm assumptions)

#Appendix A: Data Wrangling Methods
  Due to the extensiveness of the data wrangling necessary for this project, we leave the description of the process to this appendix and exclude the code generating the model ready data for either component of the project from the Code Appendix. For scripts to generate the model ready data, contact the authors of this work.
  The data released by the Midfield project was originally separated into four data tables, whose names and descriptions as well as the pertinent data to this analysis pulled from them can be found in the table below. 

```{r midtables, echo=FALSE}
midtables<-data.frame(Table = c("midfieldterms", "midfieldcourses", "midfieldstudents", "midfielddegrees"), Purpose = c("Shows the students' status by term", "Shows each instance of a student taking a class", "Shows personal information about each student", "Shows the end degree, or lack thereof, earned by each student"), UsefulData = c("Program of study by term", "Semester in which Calculus 1 was taken", "Race and Gender of each student", "Students that earned STEM degrees"))

knitr::kable(
  midtables,
  digits = 3,
  align = c('l',rep('c',2)), caption = "Midfield Data Tables"
) %>%
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    font_size = 12, latex_options = "HOLD_position")
```
  
  In order to properly combine this data for our analysis, there were several major obsticals to overcome. The first consideration was identifying the STEM programs in the data. This was done by translating the six digit cip codes into a binary variable that identified the code as either within STEM or not within STEM. [cite cip website here] This method was used for both portions of the analysis.

##Analysis Specific to Women in STEM
  While finding the proportion of women STEM graduates by institution by year was rather straightforward, there were a few modelling decisions regarding the data. The first decision was to exclude all data that came from a year and institution that graduated fewer than 10 STEM students in the given year. This was done with the intention of helping to improve the reliability of the data. 
  
  INSERT FINAL DECISION ON WHICH DATA TO SELECT HERE; UNLESS THIS IS GOING INTO THE ANOVA SECTION ABOVE

##Analysis Specific to Dropout Rates after Calculus 1
  The most complicated challenge that we faced was to identify which semester the student took their first calculus course. This issue primarily involves the midfieldcourses data table. There are two identifiers for course in this table: course title and course code. While the course title can rather clearly identify courses that could be considered Calculus 1, only three institutions, institutions c, d, and l, provided course titles in the data. 
  
  This leaves the course code as an identifier of course. These codes, however, vary by institution and are not standardized to match to particular courses. In order to approach this issue and allow more of the data to be used, we began cross-referencing subsets of course codes from particular institutions in order to attempt to match the anonymized data to a particular institution or a particular coding key in order to figure out which course code matches to Calculus 1. Note that in the attempt to create this matching, we excluded math courses from the matching set then ensured the predicted Calculus 1 course appeared in the data set. The institutions that were identified using this method were institutions a, b, e, h, j, and l. We were unable to identify the remaining institutions, so they were excluded from the second part of our analysis.  
  
  Another major considertation to make was how to define when a student drops out of the STEM field in association with Calculus 1. Noting that the data considers each year to contain six semesters, we defined a student to have dropped out of the STEM field in association with Calculus 1 if they do not graduate with a STEM degree and they do not appear in a STEM program in the fourth, fifth, or sixth semesters following the most recent taking of the course. Simply put, we sample the time period between six months and a year after the course and see whether or not the student is still in STEM. The consideration of whether or not the graduate in STEM prevents the accidental elimination of any student that gradutates immediately or shortly after Calculus 1 while the slight delay between the measurement era and the actual taking of the course gives time for the student to choose to drop out or change majors and for this change to be recorded. 

#Appendix B: Code Appendix

```{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}

```